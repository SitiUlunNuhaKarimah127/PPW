{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN3GTwyI+Z246kyXyqPWYPr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Tugas Crawling"],"metadata":{"id":"tpzcOOhrnuXm"}},{"cell_type":"markdown","source":["# 1. Install Library BeautifulSoup"],"metadata":{"id":"Svvya-2PsPSi"}},{"cell_type":"markdown","source":["Library Beautiful Soup adalah salah satu alat yang sering digunakan untuk melakukan crawling dan ekstraksi data dari dokumen HTML dan XML. Beautiful Soup menyediakan cara yang mudah untuk menavigasi dan memanipulasi struktur dokumen HTML. Ini memungkinkan Anda melakukan pencarian elemen berdasarkan tag, atribut, dan teks yang ada di dalam elemen."],"metadata":{"id":"13U8HHgNsZPx"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VwZPY--GESQj","executionInfo":{"status":"ok","timestamp":1693232339078,"user_tz":-420,"elapsed":5219,"user":{"displayName":"Siti Ulun Nuha Karimah","userId":"02748323530668587194"}},"outputId":"527fb066-8191-40ab-b1f4-2b073e1acf9e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: BeautifulSoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from BeautifulSoup4) (2.4.1)\n"]}],"source":["!pip install BeautifulSoup4"]},{"cell_type":"markdown","source":["# Import Library"],"metadata":{"id":"sTDWNVYpsfrE"}},{"cell_type":"code","source":["from typing_extensions import final\n","import requests\n","from bs4 import BeautifulSoup\n","import csv\n"],"metadata":{"id":"Q95gTAJeSjlr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. Create Crawling Function"],"metadata":{"id":"yM0kcuqdslUB"}},{"cell_type":"markdown","source":["##3.1. Fungsi get_data()\n","\n","Fungsi ini mengambil data dari halaman tugas akhir yang diberikan URL-nya. Ia mengekstrak judul tugas akhir, nama penulis, nama dosen pembimbing, dan abstrak dari halaman tersebut."],"metadata":{"id":"g6q7Nx8_sqb_"}},{"cell_type":"code","source":["# mengambil data dari website\n","def get_data (url):\n","  page = requests.get(url)\n","  if page.status_code == 200:\n","    page_html = page.content\n","    page_soup = BeautifulSoup(page_html, 'html.parser')\n","    data = []\n","\n","    container_header = page_soup.find('div', {'style' : 'float:left; width:540px;'})\n","    judul = container_header.find('a', class_='title').text.strip().replace('\\r\\n', '')\n","    person = container_header.find_all('span')\n","    abstrak = page_soup.find('p', {'align' : 'justify'}).text.strip().replace('\\r\\n', '')\n","    data.append(judul)\n","    for i in person:\n","      split_text = i.text.strip().split(':')\n","      data.append(split_text[1])\n","    data.append(abstrak)\n","    return (data)"],"metadata":{"id":"K_1tTO8ftTq_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.2. Fungsi crawling_pta()"],"metadata":{"id":"qON_m-8ntW4I"}},{"cell_type":"markdown","source":["Fungsi ini melakukan crawling atau pengambilan data dari halaman daftar tugas akhir. Ia mengambil URL halaman, mencari setiap elemen daftar tugas akhir, mengekstrak URL untuk halaman detail setiap tugas akhir, dan menggunakan fungsi get_data() untuk mendapatkan informasi lebih lanjut."],"metadata":{"id":"_lrlpUNstqUB"}},{"cell_type":"code","source":["# mengambil link selengkapnya dan juga menyimpan data\n","def crawl_pta_trunojoyo(url):\n","  response = requests.get(url)\n","\n","  if response.status_code == 200:\n","    html = response.content\n","    soup = BeautifulSoup(html, 'html.parser')\n","    next_button = soup.find_all('a', class_='pag_button')\n","    for i in next_button:\n","      if i.text.strip() == '>' :\n","        next_page_link = i.get('href')\n","    data = []\n","\n","\n","    content_page = soup.find('ul', class_='items')\n","    link_content = content_page.find_all('li')\n","    for detail in link_content:\n","      link_view = detail.find('a', class_='gray button').get('href')\n","      data.append(get_data(link_view))\n","    return(data, next_page_link)"],"metadata":{"id":"FOQd3JnofSeg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.3. Fungsi main()"],"metadata":{"id":"ywE00ZSmthjs"}},{"cell_type":"markdown","source":["Fungsi utama yang mengatur alur scraping. Ia memulai proses crawling dari URL yang diberikan sebanyak num_page halaman. Kemudian, ia menyimpan hasil crawling ke dalam file CSV."],"metadata":{"id":"VXcx2xCFtuy-"}},{"cell_type":"code","source":["# Membuat sebuah fungsi main untuk memanggil berapa page untuk crawling data\n","def main(url, num_page):\n","  next_page_link = url\n","  final_crawl = []\n","  for i in range(num_page):\n","    hasil_crawl, next_page_link = crawl_pta_trunojoyo(next_page_link)\n","    final_crawl+=hasil_crawl\n","\n","  # Membuka file CSV untuk penulisan\n","  with open('pta_trunojoyo.csv', 'w', newline='', encoding='utf-8') as csvfile:\n","    csvwriter = csv.writer(csvfile)\n","    csvwriter.writerow(['Judul', 'Penulis', 'Dosen Pembimbing I', 'Dosen Pembimbing II', 'Abstrak'])\n","    csvwriter.writerows(final_crawl)\n","  print(f'Data Berhasil Di Simpan Dengan Jumlah {len(final_crawl)} data')\n"],"metadata":{"id":"s4fR_daZfX2o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.4. Memanggil Fungsi main()"],"metadata":{"id":"jBC0oGk2t2M4"}},{"cell_type":"markdown","source":["Bagian ini menjalankan fungsi main dengan memberikan URL halaman pertama dan jumlah halaman yang ingin diambil."],"metadata":{"id":"Rt2aeyh3t63u"}},{"cell_type":"code","source":["main('https://pta.trunojoyo.ac.id/c_search/byprod/10', 1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sGUfI-1pfb61","executionInfo":{"status":"ok","timestamp":1693232348733,"user_tz":-420,"elapsed":9683,"user":{"displayName":"Siti Ulun Nuha Karimah","userId":"02748323530668587194"}},"outputId":"551f7b78-48cc-473e-a635-bfc81b74ab49"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Data Berhasil Di Simpan Dengan Jumlah 5 data\n"]}]}]}